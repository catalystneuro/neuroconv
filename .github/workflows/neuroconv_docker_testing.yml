name: NeuroConv Docker CLI tests
on:
  workflow_dispatch:
  workflow_call:
    secrets:
      AWS_ACCESS_KEY_ID:
        required: true
      AWS_SECRET_ACCESS_KEY:
        required: true
      S3_GIN_BUCKET:
        required: true

jobs:
  run:
    name: ${{ matrix.os }} Python ${{ matrix.python-version }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.12"]
        os: [ubuntu-latest] #, macos-latest, windows-latest]  # Seems docker might only be available for ubuntu on GitHub Actions
    steps:
      - uses: actions/checkout@v4
      - run: git fetch --prune --unshallow --tags
      - name: Setup Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Global Setup
        run: python -m pip install -U pip  # Official recommended way

      - name: Install pytest and neuroconv minimal
        run: |
          pip install pytest
          pip install .

      - name: Get ephy_testing_data current head hash
        id: ephys
        run: echo "::set-output name=HASH_EPHY_DATASET::$(git ls-remote https://gin.g-node.org/NeuralEnsemble/ephy_testing_data.git HEAD | cut -f1)"
      - name: Cache ephys dataset - ${{ steps.ephys.outputs.HASH_EPHY_DATASET }}
        uses: actions/cache@v4
        id: cache-ephys-datasets
        with:
          path: ./ephy_testing_data
          key: ephys-datasets-2024-08-30-${{ matrix.os }}-${{ steps.ephys.outputs.HASH_EPHY_DATASET }}
      - name: Get ophys_testing_data current head hash
        id: ophys
        run: echo "::set-output name=HASH_OPHYS_DATASET::$(git ls-remote https://gin.g-node.org/CatalystNeuro/ophys_testing_data.git HEAD | cut -f1)"
      - name: Cache ophys dataset - ${{ steps.ophys.outputs.HASH_OPHYS_DATASET }}
        uses: actions/cache@v4
        id: cache-ophys-datasets
        with:
          path: ./ophys_testing_data
          key: ophys-datasets-2022-08-18-${{ matrix.os }}-${{ steps.ophys.outputs.HASH_OPHYS_DATASET }}
      - name: Get behavior_testing_data current head hash
        id: behavior
        run: echo "::set-output name=HASH_BEHAVIOR_DATASET::$(git ls-remote https://gin.g-node.org/CatalystNeuro/behavior_testing_data.git HEAD | cut -f1)"
      - name: Cache behavior dataset - ${{ steps.behavior.outputs.HASH_BEHAVIOR_DATASET }}
        uses: actions/cache@v4
        id: cache-behavior-datasets
        with:
          path: ./behavior_testing_data
          key: behavior-datasets-2023-07-26-${{ matrix.os }}-${{ steps.behavior.outputs.HASH_behavior_DATASET }}

      - if: steps.cache-ephys-datasets.outputs.cache-hit != 'true' || steps.cache-ophys-datasets.outputs.cache-hit != 'true' || steps.cache-behavior-datasets.outputs.cache-hit != 'true'
        name: Install and configure AWS CLI
        run: |
          pip install awscli
          aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      - if: steps.cache-ephys-datasets.outputs.cache-hit != 'true'
        name: Download ephys dataset from S3
        run: aws s3 cp --recursive ${{ secrets.S3_GIN_BUCKET }}/ephy_testing_data ./ephy_testing_data
      - if: steps.cache-ophys-datasets.outputs.cache-hit != 'true'
        name: Download ophys dataset from S3
        run: aws s3 cp --recursive ${{ secrets.S3_GIN_BUCKET }}/ophys_testing_data ./ophys_testing_data
      - if: steps.cache-behavior-datasets.outputs.cache-hit != 'true'
        name: Download behavior dataset from S3
        run: aws s3 cp --recursive ${{ secrets.S3_GIN_BUCKET }}/behavior_testing_data ./behavior_testing_data

      - name: Pull docker image
        run: |
          docker pull ghcr.io/catalystneuro/neuroconv:latest
          docker pull ghcr.io/catalystneuro/neuroconv_yaml_variable:latest

      - name: Run docker tests
        run: pytest tests/docker_yaml_conversion_specification_cli.py -vv -rsx
